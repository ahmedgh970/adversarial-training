{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Autoencoders_Adversarial_Training_UAD.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "He5bnMxF4eUO",
        "pUt7KJYD4jH_",
        "BODSO6Mr4ymn",
        "ONbEgrjv55Xq",
        "A_KV8CdA6MU7",
        "UO40zWJ96_SY",
        "JBL3NyI957XU",
        "Guj7hFmq7Pjy",
        "9ytUmxAK7Rp4",
        "NeeKAOcK6Ahf",
        "nuEgxobC7T_N",
        "ob9yVvQg7V__",
        "PrHDynS67XoN",
        "QucRsBiN7b5F",
        "rx-OXyQG7eDv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He5bnMxF4eUO"
      },
      "source": [
        "# Drive mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0TlXj0x4WlK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUt7KJYD4jH_"
      },
      "source": [
        "# Installs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqXin51A4qN_"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BODSO6Mr4ymn"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiBWiMst40Zz"
      },
      "source": [
        "from art.attacks.evasion import ProjectedGradientDescent\n",
        "from art.estimators.classification import KerasClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "from skimage import filters\n",
        "from einops import rearrange\n",
        "import statistics\n",
        "import seaborn as sns; sns.set_theme()\n",
        "\n",
        "import random\n",
        "import traceback\n",
        "import nibabel as nib\n",
        "import scipy \n",
        "\n",
        "import numpy as np\n",
        "from numpy import save\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from plot_keras_history import plot_history\n",
        "\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn import metrics\n",
        "\n",
        "from scripts.evalresults import *\n",
        "from scripts.utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Crafting"
      ],
      "metadata": {
        "id": "2HsHch_kOQMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Trusted-AI/adversarial-robustness-toolbox"
      ],
      "metadata": {
        "id": "j6yg4J12Ofw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ."
      ],
      "metadata": {
        "id": "nWnFLj_bOjJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load my train and validation sets with labels\n",
        "x_train = np.load('./OASIS/healthy_oasis/x_train.npy')\n",
        "y_train = np.load('./OASIS/healthy_oasis/y_train.npy')\n",
        "\n",
        "x_train_adv = np.load('./OASIS/adv_oasis/x_train_adv.npy')\n",
        "\n",
        "# All data shape: (H=256, W=256, C=1)\n",
        "\n",
        "# Define the estimator (classifier) to generate the PGD attack \n",
        "model = tf.keras.applications.ResNet50(\n",
        "  include_top=True,\n",
        "  weights=None,\n",
        "  input_tensor=None,\n",
        "  input_shape=(256, 256,1),\n",
        "  pooling=max,\n",
        "  classes=3,\n",
        "  classifier_activation='softmax'\n",
        ")\n",
        "\n",
        "\n",
        "# Compile\n",
        "model.summary()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy']);  \n",
        "\n",
        "#model.load_weights('./saved/ResNet50/ckpt/Model_Ckpts_train.h5')\n",
        "\n",
        "\n",
        "\n",
        "#------------------- Train the classifier--------------------------------------\n",
        "# Callbacks          \n",
        "calbks = tf.keras.callbacks.ModelCheckpoint(filepath='./saved/ResNet50/ckpt/Model_Ckpts_train.h5', monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
        " \n",
        "\n",
        "# Train\n",
        "hist = model.fit(x_train, y_train, epochs=50, validation_split=0.2, callbacks=[calbks]);\n",
        "plot_history(hist, path='./saved/ResNet50/ckpt/History.png')\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# Test\n",
        "#loss_test, accuracy_test = model.evaluate(x_test, y_test)\n",
        "#print('Accuracy on testing-set: {:4.2f}%'.format(accuracy_test * 100))\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#------------------- Craft the adv samples -------------------------------------\n",
        "# Create a ART keras classifier for the TensorFlow Keras model\n",
        "classifier = KerasClassifier(model=model, clip_values=(0, 1))\n",
        "\n",
        "\n",
        "# Get PGD attack on Guassian process classification\n",
        "eps = 0.20\n",
        "attack = ProjectedGradientDescent(classifier,\n",
        "                                  eps=eps,\n",
        "                                  eps_step=eps/10,\n",
        "                                  targeted=True,\n",
        "                                  batch_size=64)\n",
        "\n",
        "\n",
        "# Generate adv examples\n",
        "x_train_adv = attack.generate(x_train)\n",
        "np.save('./saved/ResNet50/adv_data/x_train_adv.npy', x_train_adv)\n",
        "\n",
        "\n",
        "# Evaluate on adversarial train data\n",
        "loss_test, accuracy_test = model.evaluate(x_train_adv, y_train)\n",
        "perturbation = np.mean(np.abs((x_train_adv - x_train)))\n",
        "print(f'With eps=0.2 we have:\\n')\n",
        "print('Accuracy on adversarial train data: {:4.2f}%'.format(accuracy_test * 100))\n",
        "print('Average perturbation: {:4.2f}\\n'.format(perturbation))\n",
        "#------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "P_u8Qy50OUfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONbEgrjv55Xq"
      },
      "source": [
        "# Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_KV8CdA6MU7"
      },
      "source": [
        "### Dense Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DICbwKf54j3"
      },
      "source": [
        "#-- Model implementation : Dense Convolutional Autoencoder\n",
        "\n",
        "def DCAE():\n",
        "  \n",
        "    input_img = tf.keras.Input(shape=(image_size, image_size, num_channels)) \n",
        "\n",
        "    x = layers.Conv2D(32 , 5, activation=layers.LeakyReLU(), strides=2, padding=\"same\")(input_img)\n",
        "    x = layers.Conv2D(64 , 5, activation=layers.LeakyReLU(), strides=2, padding=\"same\")(x)\n",
        "    x = layers.Conv2D(128, 5, activation=layers.LeakyReLU(), strides=2, padding=\"same\")(x)    \n",
        "    x = layers.Conv2D(128, 5, activation=layers.LeakyReLU(), strides=2, padding=\"same\")(x)   \n",
        "    x = layers.Conv2D(16 , 1, activation=layers.LeakyReLU(), strides=1, padding=\"same\")(x)\n",
        "    \n",
        "    x = layers.Flatten()(x)\n",
        "    encoded = layers.Dense(intermediate_dim, activation=layers.LeakyReLU())(x)\n",
        "\n",
        "    #-- BOTTELNECK SIZE : 512 \n",
        "    \n",
        "    x = layers.Dense(16 * 16 * 16, activation=layers.LeakyReLU())(encoded)\n",
        "    x = layers.Reshape((16, 16, 16))(x)\n",
        "\n",
        "    x = layers.Conv2D(128, 1, strides=1, activation=layers.LeakyReLU(), padding=\"same\")(x)    \n",
        "    x = layers.Conv2DTranspose(128, 5, strides=2, activation=layers.LeakyReLU(), padding=\"same\")(x) \n",
        "    x = layers.Conv2DTranspose(64 , 5, strides=2, activation=layers.LeakyReLU(), padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(32 , 5, strides=2, activation=layers.LeakyReLU(), padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(32 , 5, strides=2, activation=layers.LeakyReLU(), padding=\"same\")(x)\n",
        "    \n",
        "    decoded = layers.Conv2D(num_channels, 1, activation=layers.LeakyReLU(), padding='same')(x)\n",
        "    \n",
        "    model = tf.keras.Model(input_img, decoded)\n",
        "\n",
        "    return model\n",
        "\n",
        "    \n",
        "#-- Configure the hyperparameters\n",
        "\n",
        "model_name = 'Dense Convolutional Autoencoder'\n",
        "numEpochs = 50\n",
        "learning_rate = 0.00001\n",
        "image_size = 256\n",
        "num_channels = 1\n",
        "batch_size = 1\n",
        "intermediate_dim = 512\n",
        "\n",
        "\n",
        "#-- Configure training and testing Datasets \n",
        "\n",
        "saved_dir = './saved/'\n",
        "data_dir  = './data/OASIS/'\n",
        "\n",
        "test_healthy_path = './data/OASIS_all/OASIS_Flair_Test.npy'\n",
        "\n",
        "test_path = './data/BraTS/s0/BraTS_Flair.npy'\n",
        "brainmask_path = './data/BraTS/s0/BraTS_Brainmask.npy'\n",
        "x_prior_path = './data/BraTS/s0/BraTS_prior_52.npy.npy'\n",
        "label_path = './data/BraTS/s0/BraTS_GT.npy'\n",
        "\n",
        "'''\n",
        "#-- If using MSLUB as test-set\n",
        "test_path = './data/MSLUB/MSLUB_Flair.npy'\n",
        "brainmask_path = './data/MSLUB/MSLUB_Brainmask.npy'\n",
        "x_prior_path = './data/MSLUB/MSLUB_prior_57.npy'\n",
        "label_path = './data/MSLUB/MSLUB_GT.npy'\n",
        "'''\n",
        "\n",
        "list_len = [4805, 3078]    #-- BraTS and MSLUB test-set sizes, respectively.\n",
        "len_testset = list_len[0]  #-- 0 for BraTS and 1 for MSLUB\n",
        "\n",
        "train_paths = list_of_paths(data_dir)\n",
        "\n",
        "nb_train_files = 66\n",
        "data_gen = data_generator(train_paths[:nb_train_files], batch_size)\n",
        "training_steps = (256 / batch_size) * nb_train_files\n",
        "\n",
        "nb_val_files = 5\n",
        "val_gen = data_generator(train_paths[-nb_val_files:], batch_size)\n",
        "validation_steps = (256 / batch_size) * nb_val_files\n",
        "\n",
        "\n",
        "#-- Checkpoints dir\n",
        "\n",
        "date = datetime. now(). strftime(\"%Y_%m_%d-%I:%M:%S_%p\")\n",
        "ckpts_dir = os.path.join(saved_dir, f'Ckpts_{date}')\n",
        "os.makedirs(ckpts_dir)\n",
        "      \n",
        "ckpts_path = os.path.join(ckpts_dir, 'Model_Ckpts.h5')\n",
        "params_path = os.path.join(ckpts_dir, 'Parameters.txt')\n",
        "results_path = os.path.join(ckpts_dir, 'Results.txt')\n",
        "fig_path = os.path.join(ckpts_dir, 'History_plot.png')\n",
        "dice_plot_path = os.path.join(ckpts_dir, 'Dice_plot.png')\n",
        "predicted_path = os.path.join(ckpts_dir, 'Predicted.npy')\n",
        "residual_path = os.path.join(ckpts_dir, 'Residuals.npy')\n",
        "residual_BP_path = os.path.join(ckpts_dir, 'Residuals_BP.npy')\n",
        " \n",
        "\n",
        "#-- Configure the training\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "          \n",
        "calbks = tf.keras.callbacks.ModelCheckpoint(filepath=ckpts_path, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=2)\n",
        "tqdm_callback = tfa.callbacks.TQDMProgressBar() \n",
        "\n",
        "model = DCAE()\n",
        "model.summary()\n",
        "model.compile(optimizer=opt, loss='mae', metrics=['mse', SSIMLoss, MS_SSIMLoss])\n",
        "\n",
        "\n",
        "#-- Print & Write model Parameters\n",
        "\n",
        "parameters = (f'\\nSelected model \"{model_name}\" with :\\n - {batch_size}: Batche(s)\\n - {numEpochs}: Epochs\\n - {intermediate_dim}: Bottelneck size\\n')\n",
        "print(parameters)\n",
        "\n",
        "           \n",
        "#-- TRAIN \n",
        "\n",
        "print('\\nTrain =>\\n')\n",
        "history = model.fit(x = data_gen,\n",
        "                    steps_per_epoch = training_steps,\n",
        "                    validation_data = val_gen,\n",
        "                    validation_steps = validation_steps,\n",
        "                    verbose = 0,\n",
        "                    epochs = numEpochs,\n",
        "                    callbacks = [calbks, tqdm_callback]\n",
        "                    )\n",
        "\n",
        "                          \n",
        "#-- Get training and test loss histories \n",
        "\n",
        "plot_history(history, path=fig_path)\n",
        "plt.close()\n",
        "time.sleep(2)\n",
        "\n",
        "\n",
        "#-- Test  \n",
        "\n",
        "print('\\nTest ===>\\n')\n",
        "my_test = np.load(test_path)\n",
        "brainmask = np.load(brainmask_path)\n",
        "x_prior = np.load(x_prior_path)\n",
        "my_labels = np.load(label_path)\n",
        "\n",
        "healthy_test = np.load(test_healthy_path)\n",
        "steps = healthy_test.shape[0]\n",
        "\n",
        "score = model.evaluate(x=healthy_test, y=healthy_test, verbose=0, steps=steps, callbacks = [tqdm_callback])    \n",
        "score_out = (f'\\nTest on healthy unseen data :\\n - Test loss (MAE): {score[0]},\\n - Test MSE : {score[1]},\\n - Test SSIM : {score[2]}\\n - Test MS_SSIM : {score[3]}\\n')\n",
        "print(score_out)\n",
        "\n",
        "\n",
        "#-- Predict\n",
        "\n",
        "print('\\nPredict =====>\\n')\n",
        "predicted = model.predict(x=my_test, verbose=1, steps=len_testset)\n",
        "np.save(predicted_path, predicted)\n",
        "time.sleep(4)\n",
        "\n",
        "\n",
        "#-- Calculate, Post-process and Save Residuals\n",
        "\n",
        "print('\\nCalculate, Post-process and Save Residuals =====>\\n')     \n",
        "residual_BP = calculate_residual_BP(my_test, predicted, brainmask)  #-- You can use either brainmask or x_prior\n",
        "np.save(residual_BP_path, residual_BP)\n",
        "        \n",
        "residual = calculate_residual(my_test, predicted, brainmask)  #-- You can use either brainmask or x_prior\n",
        "np.save(residual_path, residual)\n",
        "        \n",
        "\n",
        "#-- Evaluation\n",
        "\n",
        "print('\\nEvaluate =========>\\n')        \n",
        "[AUROC, AUPRC, AVG_DICE, MAD, STD], DICE = eval_residuals(my_labels, residual)     \n",
        "results = (f'\\nResults after median_filter :\\n - AUROC = {AUROC}\\n - AUPRC = {AUPRC}\\n - AVG_DICE = {AVG_DICE}\\n - MEDIAN_ABSOLUTE_DEVIATION = {MAD}\\n - STANDARD_DEVIATION = {STD}')\n",
        "print(results)\n",
        "                      \n",
        "plt.figure()\n",
        "hor_axis = [x for x in range(len_testset)]\n",
        "plt.scatter(hor_axis, DICE, s = 5, marker = '.', c = 'blue')\n",
        "plt.ylabel('Dice Score')\n",
        "plt.xlabel('N째 Samples')\n",
        "plt.title('Dice scores')\n",
        "plt.savefig(dice_plot_path)\n",
        "time.sleep(2)\n",
        "\n",
        "\n",
        "#-- Save\n",
        "\n",
        "print('\\nSave Results and Parameters =============>\\n')\n",
        "f = open(results_path, \"w\")\n",
        "f.write(results)       \n",
        "f.close()   \n",
        "                         \n",
        "f = open(params_path, \"w\")\n",
        "f.write(parameters)\n",
        "f.write(score_out)\n",
        "f.close()\n",
        "\n",
        "\n",
        "#-- End\n",
        "\n",
        "print('\\nEnd !\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBL3NyI957XU"
      },
      "source": [
        "# Latent Variable Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Guj7hFmq7Pjy"
      },
      "source": [
        "### Variational Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipxZQ5kC5-cr"
      },
      "source": [
        "#-- Model implementation : Variational Autoencoder\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "        \n",
        "\n",
        "def VAE():\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(image_size, image_size, num_channels))\n",
        "  \n",
        "  x = layers.Conv2D(32 , 5, activation=layers.LeakyReLU(), strides=2, padding=\"same\")(inputs)\n",
        "  x = layers.Conv2D(64 , 5, activation=layers.LeakyReLU(), strides=2, padding=\"same\")(x)\n",
        "  x = layers.Conv2D(128, 5, activation=layers.LeakyReLU(), strides=2, padding=\"same\")(x)    \n",
        "  x = layers.Conv2D(128, 5, activation=layers.LeakyReLU(), strides=2, padding=\"same\")(x)   \n",
        "  x = layers.Conv2D(16 , 1, activation=layers.LeakyReLU(), strides=1, padding=\"same\")(x)   \n",
        "  x = layers.Flatten()(x)\n",
        "  encoded = layers.Dense(intermediate_dim, activation=layers.LeakyReLU())(x)   \n",
        "  \n",
        "  z_mean = layers.Dense(latent_dim, name=\"z_mean\")(encoded)\n",
        "  z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(encoded)\n",
        "  z = Sampling()([z_mean, z_log_var])\n",
        "  \n",
        "  x = layers.Dense(16 * 16 * 16, activation=layers.LeakyReLU())(z)\n",
        "  x = layers.Reshape((16, 16, 16))(x)\n",
        "  x = layers.Conv2D(128, 1, strides=1, activation=layers.LeakyReLU(), padding=\"same\")(x)    \n",
        "  x = layers.Conv2DTranspose(128, 5, strides=2, activation=layers.LeakyReLU(), padding=\"same\")(x) \n",
        "  x = layers.Conv2DTranspose(64 , 5, strides=2, activation=layers.LeakyReLU(), padding=\"same\")(x)\n",
        "  x = layers.Conv2DTranspose(32 , 5, strides=2, activation=layers.LeakyReLU(), padding=\"same\")(x)\n",
        "  x = layers.Conv2DTranspose(32 , 5, strides=2, activation=layers.LeakyReLU(), padding=\"same\")(x)\n",
        "  decoder_outputs = layers.Conv2D(num_channels, 1, activation=layers.LeakyReLU(), padding='same')(x)\n",
        "  \n",
        "  model = tf.keras.Model(inputs, decoder_outputs)\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "#-- Configure the hyperparameters\n",
        "\n",
        "model_name = 'Variational Autoencoder'\n",
        "numEpochs = 50\n",
        "learning_rate = 0.00001\n",
        "rate = 0.  \n",
        "image_size = 256\n",
        "num_channels = 1\n",
        "batch_size = 1\n",
        "latent_dim = 512\n",
        "intermediate_dim = 2048\n",
        "\n",
        "\n",
        "#-- Configure training and testing Datasets \n",
        "\n",
        "saved_dir = './saved/'\n",
        "data_dir  = './data/OASIS/'\n",
        "\n",
        "test_healthy_path = './data/OASIS_all/OASIS_Flair_Test.npy'\n",
        "\n",
        "test_path = './data/BraTS/s0/BraTS_Flair.npy'\n",
        "brainmask_path = './data/BraTS/s0/BraTS_Brainmask.npy'\n",
        "x_prior_path = './data/BraTS/s0/BraTS_prior_52.npy.npy' \n",
        "label_path = './data/BraTS/s0/BraTS_GT.npy'\n",
        "\n",
        "'''\n",
        "#-- If using MSLUB as test-set\n",
        "test_path = './data/MSLUB/MSLUB_Flair.npy'\n",
        "brainmask_path = './data/MSLUB/MSLUB_Brainmask.npy'\n",
        "x_prior_path = './data/MSLUB/MSLUB_prior_57.npy'    \n",
        "label_path = './data/MSLUB/MSLUB_GT.npy'\n",
        "'''\n",
        "\n",
        "list_len = [4805, 3078]    #-- BraTS and MSLUB test-set sizes, respectively.\n",
        "len_testset = list_len[0]  #-- 0 for BraTS and 1 for MSLUB\n",
        "\n",
        "train_paths = list_of_paths(data_dir)\n",
        "\n",
        "nb_train_files = 66\n",
        "data_gen = data_generator(train_paths[:nb_train_files], batch_size)\n",
        "training_steps = (256 / batch_size) * nb_train_files\n",
        "\n",
        "nb_val_files = 5\n",
        "val_gen = data_generator(train_paths[-nb_val_files:], batch_size)\n",
        "validation_steps = (256 / batch_size) * nb_val_files\n",
        "\n",
        "\n",
        "#-- Checkpoints dir\n",
        "\n",
        "date = datetime. now(). strftime(\"%Y_%m_%d-%I:%M:%S_%p\")\n",
        "ckpts_dir = os.path.join(saved_dir, f'Ckpts_{date}')\n",
        "os.makedirs(ckpts_dir)\n",
        "     \n",
        "ckpts_path = os.path.join(ckpts_dir, 'Model_Ckpts.h5')\n",
        "params_path = os.path.join(ckpts_dir, 'Parameters.txt')\n",
        "results_path = os.path.join(ckpts_dir, 'Results.txt')\n",
        "fig_path = os.path.join(ckpts_dir, 'History_plot.png')\n",
        "dice_plot_path = os.path.join(ckpts_dir, 'Dice_plot.png')\n",
        "predicted_path = os.path.join(ckpts_dir, 'Predicted.npy')\n",
        "residual_path = os.path.join(ckpts_dir, 'Residuals.npy')\n",
        "residual_BP_path = os.path.join(ckpts_dir, 'Residuals_BP.npy')\n",
        "\n",
        "      \n",
        "#-- Configure the training\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "           \n",
        "calbks = tf.keras.callbacks.ModelCheckpoint(filepath=ckpts_path, monitor='loss', save_best_only=True, save_weights_only=True, verbose=2)\n",
        "tqdm_callback = tfa.callbacks.TQDMProgressBar() \n",
        "\n",
        "model = VAE()\n",
        "model.summary()\n",
        "model.compile(optimizer=opt, loss='mae', metrics=['mse', SSIMLoss, MS_SSIMLoss])\n",
        "\n",
        "\n",
        "#-- Print & Write model Parameters\n",
        "\n",
        "parameters = (f'\\nSelected model \"{model_name}\" with :\\n - {batch_size}: Batche(s)\\n - {numEpochs}: Epochs\\n - {intermediate_dim}: Intermediate dim\\n - {latent_dim}: Bottelneck size\\n')\n",
        "print(parameters)\n",
        "\n",
        "  \n",
        "#-- TRAIN \n",
        "\n",
        "print('\\nTrain =>\\n')\n",
        "history = model.fit(x = data_gen,\n",
        "                    steps_per_epoch = training_steps,\n",
        "                    validation_data = val_gen,\n",
        "                    validation_steps = validation_steps,\n",
        "                    verbose = 0,\n",
        "                    epochs = numEpochs,\n",
        "                    callbacks = [calbks, tqdm_callback]\n",
        "                    )\n",
        "\n",
        "                          \n",
        "#-- Get training and test loss histories \n",
        "\n",
        "plot_history(history, path=fig_path)\n",
        "plt.close()\n",
        "time.sleep(2)\n",
        "\n",
        "\n",
        "#-- Test\n",
        "\n",
        "print('\\nTest ===>\\n')\n",
        "my_test = np.load(test_path)\n",
        "brainmask = np.load(brainmask_path)\n",
        "x_prior = np.load(x_prior_path)\n",
        "my_labels = np.load(label_path)\n",
        "\n",
        "healthy_test = np.load(test_healthy_path)\n",
        "steps = healthy_test.shape[0]\n",
        "\n",
        "score = model.evaluate(x=healthy_test, y=healthy_test, verbose=0, steps=steps, callbacks = [tqdm_callback])    \n",
        "score_out = (f'\\nTest on healthy unseen data :\\n - Test loss (MAE): {score[0]},\\n - Test MSE : {score[1]},\\n - Test SSIM : {score[2]}\\n - Test MS_SSIM : {score[3]}\\n')\n",
        "print(score_out)\n",
        "\n",
        "\n",
        "#-- Predict\n",
        "\n",
        "print('\\nPredict =====>\\n')\n",
        "predicted = model.predict(x=my_test, verbose=1, steps=len_testset)\n",
        "np.save(predicted_path, predicted)\n",
        "time.sleep(4)\n",
        "\n",
        "\n",
        "#-- Calculate, Post-process and Save Residuals\n",
        "\n",
        "print('\\nCalculate, Post-process and Save Residuals =====>\\n')     \n",
        "residual_BP = calculate_residual_BP(my_test, predicted, brainmask)  #-- You can use either brainmask or x_prior\n",
        "np.save(residual_BP_path, residual_BP)\n",
        "        \n",
        "residual = calculate_residual(my_test, predicted, brainmask)  #-- You can use either brainmask or x_prior\n",
        "np.save(residual_path, residual)\n",
        "        \n",
        "\n",
        "#-- Evaluation\n",
        "\n",
        "print('\\nEvaluate =========>\\n')        \n",
        "[AUROC, AUPRC, AVG_DICE, MAD, STD], DICE = eval_residuals(my_labels, residual)     \n",
        "results = (f'\\nResults after median_filter :\\n - AUROC = {AUROC}\\n - AUPRC = {AUPRC}\\n - AVG_DICE = {AVG_DICE}\\n - MEDIAN_ABSOLUTE_DEVIATION = {MAD}\\n - STANDARD_DEVIATION = {STD}')\n",
        "print(results)\n",
        "                      \n",
        "plt.figure()\n",
        "hor_axis = [x for x in range(len_testset)]\n",
        "plt.scatter(hor_axis, DICE, s = 5, marker = '.', c = 'blue')\n",
        "plt.ylabel('Dice Score')\n",
        "plt.xlabel('N째 Samples')\n",
        "plt.title('Dice scores')\n",
        "plt.savefig(dice_plot_path)\n",
        "time.sleep(2)\n",
        "\n",
        "\n",
        "#-- Save\n",
        "\n",
        "print('\\nSave Results and Parameters =============>\\n')\n",
        "f = open(results_path, \"w\")\n",
        "f.write(results)       \n",
        "f.close()   \n",
        "                       \n",
        "f = open(params_path, \"w\")\n",
        "f.write(parameters)\n",
        "f.write(score_out)\n",
        "f.close()\n",
        "\n",
        "      \n",
        "#-- End\n",
        "\n",
        "print('\\nEnd !\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "XIu2GK48O71n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate on MSLUB Dataset"
      ],
      "metadata": {
        "id": "MkLomYJcO9ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_ckpts_dir = ['./saved/DCAE/Predicted/mslub/', './saved/VAE/Predicted/mslub/'] \n",
        "\n",
        "for l in list_ckpts_dir:\n",
        "\n",
        "  ckpts_dir = l\n",
        "  results_path = os.path.join(ckpts_dir, 'Results.txt')\n",
        "  dice_plot_path = os.path.join(ckpts_dir, 'Dice_plot.png')\n",
        "  predicted_path = os.path.join(ckpts_dir, 'Predicted_mslub.npy')\n",
        "  residual57_path = os.path.join(ckpts_dir, 'Residuals57.npy')\n",
        "  residual84_path = os.path.join(ckpts_dir, 'Residuals84.npy')\n",
        "  residual_BP_path = os.path.join(ckpts_dir, 'Residuals_BP.npy')\n",
        "\n",
        "  test_path = './data/MSLUB/MSLUB_Flair.npy'\n",
        "  prior57_path = './data/MSLUB/MSLUB_prior_57.npy'\n",
        "  prior84_path = './data/MSLUB/MSLUB_prior_84.npy'\n",
        "  label_path = './data/MSLUB/MSLUB_GT.npy'\n",
        "  brainmask_path = './data/MSLUB/MSLUB_Brainmask.npy'\n",
        "\n",
        "  # Test       \n",
        "  print('\\nTest ===>\\n')\n",
        "  predicted = np.load(predicted_path)\n",
        "  my_test = np.load(test_path)\n",
        "  brainmask = np.load(brainmask_path)\n",
        "  my_labels = np.load(label_path)\n",
        "  prior57 = np.load(prior57_path)\n",
        "  prior84 = np.load(prior84_path)\n",
        "\n",
        "  #-- Calculate, Post-process and Save Residuals\n",
        "  print('\\nCalculate, Post-process and Save Residuals =====>\\n')     \n",
        "  residual_BP = calculate_residual_BP(my_test, predicted, brainmask)\n",
        "  np.save(residual_BP_path, residual_BP)\n",
        "        \n",
        "  residual_57 = calculate_residual(my_test, predicted, prior57)\n",
        "  np.save(residual57_path, residual_57)\n",
        "\n",
        "  residual_84 = calculate_residual(my_test, predicted, prior84)\n",
        "  np.save(residual84_path, residual_84)\n",
        "        \n",
        "\n",
        "  #-- Evaluation\n",
        "  print('\\nEvaluate =========>\\n')        \n",
        "  [AUROC, AUPRC, AVG_DICE, MAD, STD], DICE = eval_residuals(my_labels, residual_57)     \n",
        "  results_57 = (f'\\nResults after median_filter and x_prior57 :\\n - AUROC = {AUROC}\\n - AUPRC = {AUPRC}\\n - AVG_DICE = {AVG_DICE}\\n - MEDIAN_ABSOLUTE_DEVIATION = {MAD}\\n - STANDARD_DEVIATION = {STD}')\n",
        "  print(results_57)\n",
        "  \n",
        "  [AUROC, AUPRC, AVG_DICE, MAD, STD], DICE = eval_residuals(my_labels, residual_84)     \n",
        "  results_84 = (f'\\nResults after median_filter and x_prior84 :\\n - AUROC = {AUROC}\\n - AUPRC = {AUPRC}\\n - AVG_DICE = {AVG_DICE}\\n - MEDIAN_ABSOLUTE_DEVIATION = {MAD}\\n - STANDARD_DEVIATION = {STD}')\n",
        "  print(results_84)\n",
        "      \n",
        "  len_testset = my_labels.shape[0]\n",
        "               \n",
        "  plt.figure()\n",
        "  hor_axis = [x for x in range(len_testset)]\n",
        "  plt.scatter(hor_axis, DICE, s = 5, marker = '.', c = 'blue')\n",
        "  plt.ylabel('Dice Score')\n",
        "  plt.xlabel('N째 Samples')\n",
        "  plt.title('Dice scores')\n",
        "  plt.savefig(dice_plot_path)\n",
        "  time.sleep(2)\n",
        "\n",
        "  #-- Save\n",
        "  print('\\nSave Results and Parameters =============>\\n')\n",
        "  f = open(results_path, \"w\")\n",
        "  f.write(results_57)\n",
        "  f.write(results_84)       \n",
        "  f.close()   \n",
        "       \n",
        "  print('\\nEnd of evaluation !\\n')"
      ],
      "metadata": {
        "id": "C8Wt5yT3PGA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate on BraTS Dataset"
      ],
      "metadata": {
        "id": "aSqOIDz9PEAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_ckpts_dir = ['./saved/DCAE/Predicted/brats/', './saved/VAE/Predicted/brats/']     \n",
        "\n",
        "\n",
        "for l in list_ckpts_dir:\n",
        "\n",
        "  ckpts_dir = l\n",
        "  results_path = os.path.join(ckpts_dir, 'Results.txt')\n",
        "  dice_plot_path = os.path.join(ckpts_dir, 'Dice_plot.png')\n",
        "  predicted_path = os.path.join(ckpts_dir, 'Predicted_brats.npy')\n",
        "  residual_path = os.path.join(ckpts_dir, 'Residuals.npy')\n",
        "  residual_BP_path = os.path.join(ckpts_dir, 'Residuals_BP.npy')\n",
        "\n",
        "  test_path = './data/BraTS/BraTS_Flair.npy'\n",
        "  prior_path = './data/BraTS/BraTS_prior_52.npy'\n",
        "  label_path = './data/BraTS/BraTS_GT.npy'\n",
        "  brainmask_path = './data/BraTS/BraTS_Brainmask.npy'\n",
        "\n",
        "  # Test       \n",
        "  print('\\nTest ===>\\n')\n",
        "  predicted = np.load(predicted_path)\n",
        "  my_test = np.load(test_path)\n",
        "  brainmask = np.load(brainmask_path)\n",
        "  my_labels = np.load(label_path)\n",
        "  prior = np.load(prior_path)\n",
        "\n",
        "  #-- Calculate, Post-process and Save Residuals\n",
        "  print('\\nCalculate, Post-process and Save Residuals =====>\\n')     \n",
        "  residual_BP = calculate_residual_BP(my_test, predicted, brainmask)\n",
        "  np.save(residual_BP_path, residual_BP)\n",
        "        \n",
        "  residual = calculate_residual(my_test, predicted, prior)\n",
        "  np.save(residual_path, residual)\n",
        "        \n",
        "\n",
        "  #-- Evaluation\n",
        "  print('\\nEvaluate =========>\\n')        \n",
        "  [AUROC, AUPRC, AVG_DICE, MAD, STD], DICE = eval_residuals(my_labels, residual)     \n",
        "  results = (f'\\nResults after median_filter :\\n - AUROC = {AUROC}\\n - AUPRC = {AUPRC}\\n - AVG_DICE = {AVG_DICE}\\n - MEDIAN_ABSOLUTE_DEVIATION = {MAD}\\n - STANDARD_DEVIATION = {STD}')\n",
        "  print(results)\n",
        "      \n",
        "  len_testset = my_labels.shape[0]\n",
        "               \n",
        "  plt.figure()\n",
        "  hor_axis = [x for x in range(len_testset)]\n",
        "  plt.scatter(hor_axis, DICE, s = 5, marker = '.', c = 'blue')\n",
        "  plt.ylabel('Dice Score')\n",
        "  plt.xlabel('N째 Samples')\n",
        "  plt.title('Dice scores')\n",
        "  plt.savefig(dice_plot_path)\n",
        "  time.sleep(2)\n",
        "\n",
        "  #-- Save\n",
        "  print('\\nSave Results and Parameters =============>\\n')\n",
        "  f = open(results_path, \"w\")\n",
        "  f.write(results)       \n",
        "  f.close()   \n",
        "       \n",
        "  #-- End\n",
        "  print('\\nEnd of evaluation !\\n')"
      ],
      "metadata": {
        "id": "p3r6lnM_PWbM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}